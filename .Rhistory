modFit <- train(clusters ~., data=ss,method ='rpart')
table(predict(modFit,training),training$Species)
library(caret)
modFit <- train(clusters ~., data=ss,method ='rpart')
head(ss)
modFit <- train(clusters ~., data=training,method ='rpart')
names(training)
str(training)
list(training$clusters)
training[is.na(training)] <- 0
modFit <- train(clusters ~., data=training,method ='rpart')
na.exclude(training)
training<-na.exclude(training)
names(training)
modFit <- train(clusters ~., data=training,method ='rpart')
library(rpart)
modFit <- train(clusters ~., data=training,method ='rpart')
library(AppliedPredictiveModeling)
?AppliedPredictiveModeling
install.packages(AppliedPredictiveModeling)
install.packages('AppliedPredictiveModeling')
install.packages("AppliedPredictiveModeling")
?AppliedPredictiveModeling
library(AppliedPredictiveModeling)
?AppliedPredictiveModeling
library(caret)
?caret
??caret
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(gbm)
library(lubridate)
library(forecast)
library(e1071)
install.packages('forecast')
library(forecast)
?accuracy
data(vowel.train)
data(vowel.test)
head(vowel.train)
vowel.train$y<-as.factor(vowel.train$y)
vowel.vowel$y<-as.factor(vowel.vowel$y)
vowel.test$y<-as.factor(vowel.test$y)
set.seed(33833)
rf_mod <- train(y~., data=vowel.traing,method='rf',prox=TRUE)
rf_mod <- train(y~., data=vowel.train,method='rf',prox=TRUE)
rf_outcome <- predict(rf_mod,vowel.test)
head(rf_outcome)
rf_accuracy <- length(rf_outcome == voweel.test$y)/length(voweel.test$y)
rf_accuracy <- length(rf_outcome == vowel.test$y)/length(vowel.test$y)
rf_accuracy
length(rf_outcome == vowel.test$y)
length(vowel.test$y)
length(rf_outcome[rf_outcome== vowel.test$y])
rf_accuracy <- length(rf_outcome[rf_outcome== vowel.test$y])/length(vowel.test$y)
rf_accuracy
gbm_mod <-train(y~., method="gbm", data=vowel.train, verbose=FALSE)
gbm_mod <-train(y~., method="gbm", data=vowel.train, verbose=FALSE)
install.packages('gbm')
install.packages("gbm")
gbm_mod <-train(y~., method="gbm", data=vowel.train, verbose=FALSE)
library(caret)
gbm_mod <-train(y~., method="gbm", data=vowel.train, verbose=FALSE)
gbm_mod <-train(y~., method="gbm", data=vowel.train)
library(gbm)
library(gbm)
gbm_mod <-train(y~., method="gbm", data=vowel.train, verbose=FALSE)
gbm_outcome <- predict(gbm_mod,vowel.test)
gbm_accuracy <- length(gbm_outcome[gbm_outcome== vowel.test$y])/length(vowel.test$y)
gbm_accuracy
vowel.test[(gbm_outcome== vowel.test$y)&(rf_outcome== vowel.test$y)]
gbm_index <- gbm_outcome== vowel.test$y
head(gbm_index)
rf_index <- rf_outcome== vowel.test$y
rf_index$gbm_index
rf_index$$gbm_index
rf_index&gbm_index
vowel.test[rf_index&gbm_index]
vowel.test$y[rf_index&gbm_index]
length(vowel.test$y[rf_index&gbm_index])/length(vowel.test$y)
length(vowel.test$y[rf_index|gbm_index])/length(vowel.test$y)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rf_mod <- train(diagnosis~., data=training, method='rf',prox=TRUE)
rf_outcome <- predict(rf_mod,testing)
gbm_mod <- train(diagnosis~., data=training, method='gbm',prox=TRUE)
gbm_outcome <- predict(gbm_mod,testing)
head(training)
gbm_mod <- train(diagnosis~., data=training, method='gbm',verbose=FALSE)
gbm_outcome <- predict(gbm_mod,testing)
lda_mod <- train(diagnosis~., data=training, method='lda')
lda_outcome <- predict(lda_mod,testing)
combine_df <- data.frame(rf_outcome,gbm_outcome,lda_outcome,diagnosis= testing$diagnosis )
combine_mod <- train(diagnosis~., method='rf', data=combine_df, prox=TRUE)
combine_outcome <- predict(combine_mod, combine_df)
combine_accuracy<-length(combine_outcome[combine_outcome==testing$diagnosis])/length(testing$diagnosis)
combine_accuracy
gbm_accuracy<-length(bgm_outcome[gbm_outcome==testing$diagnosis])/length(testing$diagnosis)
gbm_accuracy<-length(gbm_outcome[gbm_outcome==testing$diagnosis])/length(testing$diagnosis)
gbm_accuracy
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
str(training)
?lasso
??lasso
?plot.enet
??plot.eneet
??plot.enet
?plot.enet
ga_mod <- train(CompressiveStrength~., data=training, method='gasso')
?gasso
??gasso
install.packages(gasso)
install.packages('gasso')
mod_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
la_mod <- train(CompressiveStrength~., data=training, method='lasso')
library(elasticnet)
la_mod
la_mod$finalModel
plot.enet(la_mod$finalModel, xvar = "penalty", use.color = TRUE)
library(lubridate)
getwd()
dat = read.csv("~/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library("forecast")
?bats
?forecast
dim(testing)
dim(testing)[1]
mod_ts <- bats(tstrain)
fcast <- forecast(mod_ts, level = 95, h = dim(testing)[1])
fcast
str(tstrain)
head(tstrain)
tstrain[100:110]
class(tstrain)
fcast$lower
fcast$lower < testing$visitsTumblr
sum(fcast$lower < testing$visitsTumblr & testing$visitsTumblr < fcast$upper) /
dim(testing)[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testin
testing = concrete[ -inTrain,]
set.seed(325)
install.packages(e1071)
install.packages('e1071')
install.packages("e1071")
library(e1071)
?e1071
??e1071
mod_svm <- svm(CompressiveStrength ~ ., data = training)
pred_svm <- predict(mod_svm, testing)
accuracy(pred_svm, testing$CompressiveStrength)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rf_mod <- train(diagnosis~., data=training, method='rf',prox=TRUE)
rf_outcome <- predict(rf_mod,testing)
gbm_mod <- train(diagnosis~., data=training, method='gbm',verbose=FALSE)
gbm_outcome <- predict(gbm_mod,testing)
lda_mod <- train(diagnosis~., data=training, method='lda')
lda_outcome <- predict(lda_mod,testing)
combine_df <- data.frame(rf_outcome,gbm_outcome,lda_outcome,diagnosis= testing$diagnosis )
combine_mod <- train(diagnosis~., method='rf', data=combine_df, prox=TRUE)
combine_outcome <- predict(combine_mod, combine_df)
combine_accuracy<-length(combine_outcome[combine_outcome==testing$diagnosis])/length(testing$diagnosis)
confusionMatrix(combine_mod, testing$diagnosis)$overall[1]
confusionMatrix(combine_mod, testing$diagnosis)
confusionMatrix(combine_mod, testing$diagnosis)$overall
confusionMatrix(combine_mod, testing$diagnosis).overall
?confusionMatrix
confusionMatrix(combine_mod)
confusionMatrix(combine_outcome, testing$diagnosis)$overall[1]
combine_accuracy
getwd()
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/freefrog/Studing/DataScience/Practical_ML)
Train_Data <- read.csv('pml-training.csv')
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/freefrog/Studing/DataScience/Practical_ML')
Train_Data <- read.csv('pml-training.csv')
Test_Data <- read.csv('pml-testing.csv')
dim(Train_Data);dim(Test_Data)
features <- names(Train_Data[,colSums(is.na(Train_Data))==0])[8:59]
Train_Dt <- Train_Data[,c(features,"classe")]
Test_Dt <- Test_Data[,c(features,"problem_id")]
dim(Train_Dt);dim(Test_Dt);
library(tree)
install.packages('tree')
library(tree)
library(tree)
install.packages('tree')
library(tree)
library(tree)
set.seed(12345)
Model_Tree=tree(classe~.,data=Training_Data)
Model_Tree=tree(classe~.,data=Training_Data)
dim(Training_Data)
inTrain <- createDataPartition(y=Train_Data$classe, p=0.6, list=FALSE)
Training_Data <- Train_Dt[inTrain,]
Testing_Data <- Train_Dt[-inTrain,]
library(caret)
inTrain <- createDataPartition(y=Train_Data$classe, p=0.6, list=FALSE)
Training_Data <- Train_Dt[inTrain,]
Testing_Data <- Train_Dt[-inTrain,]
Model_Tree=tree(classe~.,data=Training_Data)
dim(Training_Data)
features <- names(Train_Data[,colSums(is.na(Train_Data))==0])
featureees
features
reemovee
remove
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
remove
features %in% remove
remove %in% features
which(features %in% remove)
Training <- Train_Data[, features]
dim(Training)
Training <- Train_Data[,colSums(is.na(Train_Data))==0]
dim(Training)
Train_Data <- read.csv('pml-training.csv',na.strings=c("", "NA", "NULL"))
Test_Data <- read.csv('pml-testing.csv', na.strings=c("", "NA", "NULL"))
dim(Train_Data);dim(Test_Data)
setwd('/Users/freefrog/Studing/DataScience/Practical_ML')
Train_Data <- read.csv('pml-training.csv',na.strings=c("", "NA", "NULL"))
Test_Data <- read.csv('pml-testing.csv', na.strings=c("", "NA", "NULL"))
dim(Train_Data);dim(Test_Data)
Training <- Train_Data[,colSums(is.na(Train_Data))==0]
dim(Training)
Training <- Training[,-remove]
Training <- Training[,-c(remove)]
Training <- Training[,remove]
dim(Training)
which(Training[,remove])
which(nmaes(Training)==remove)
which(names(Training)==remove)
Training <- Train_Data[,colSums(is.na(Train_Data))==0]
Training <- Training[,-which(names(Training)==remove)]
Training[,-which(names(Training)==remove)]
Training <- Train_Data[,colSums(is.na(Train_Data))==0]
dim(Training)
-which(names(Training)==remove)
-which(names(Training) %in% remove)
Training[,-which(names(Training) %in% remove)]
Training <- Training[,-which(names(Training) %in% remove)]
dim(Training)
library(caret)
sapply(Training, is.numeric)
Training[sapply(Training, is.numeric)]
Training[,sapply(Training, is.numeric)]
zeroVar= nearZeroVar(Training[,sapply(Training, is.numeric)],
saveMetrics = TRUE)
zeroVar
zeroVar= nearZeroVar(Training[,sapply(Training, is.numeric)],
saveMetrics = FALSE)
zeroVar
zeroVar= nearZeroVar(Training[,sapply(Training, is.numeric)],
saveMetrics = TRUE)
dim(zeroVar)
dim(Training[,sapply(Training, is.numeric)])
sss = Training[,zeroVar[, 'nzv']==0]
sss
dim(sss)
zeroVar[, 'nzv']==0
Training = Training[,zeroVar[, 'nzv']==0]
cor(na.omit(Training[sapply(Training, is.numeric)]))
aaa<-cor(na.omit(Training[sapply(Training, is.numeric)]))
bbb<-cor(Training[sapply(Training, is.numeric)])
summary(aaa)
summary(bbb)
identical(aaa,bbb)
corrMatrix <- cor(Training[sapply(Training, is.numeric)])
dim(corrMatrix)
as.vector(corrMatrix)
corrDF <- expand.grid(row = 1:dim(corrMatrix ),
col = 1:dim(corrMatrix ))
dim(corrMatrix )
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1],
col = 1:dim(corrMatrix)[2])
corrDF
summary(corrDF)
summary(corrMatrix)
?cor
test <- data.frame(x=c(1,2,3,4), y=c(4,5,6,7))
test
cor(test)
corr[1,1]
corrMatrix[1,1]
corrMatrix[1,2]
corrDF$correlation <- corrMatrix
as.vector(test)
class(as.vector(test))
class(as.vector(test)[1])
?as.vector
is.vector(corrMatrix)
is.list(corrMatrix)
class(corrMatrix)
?matrix
matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3)
aaa <- matrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3)
aaa
as.vector(aaa)
dim(as.vector(corrMatrix))
length(as.vector(corrMatrix))
dim(corrMatrix)[1]*dim(corrMatrix[2])
dim(corrMatrix)[1].*dim(corrMatrix[2])
dim(corrMatrix)[1]
52*52
?levelplot
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
levelplot(correlation ~ row, corrDF)
levelplot(correlation ~ row+col, corrDF)
corrDF
summary(corrDF)
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1],
col = 1:dim(corrMatrix)[2])
dim(corrDF)
head(corrDF)
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1],
col = 1:dim(corrMatrix)[2])
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
findCorrelation(corrMatrix, cutoff = .90, verbose = TRUE)
High_cor_var = findCorrelation(corrMatrix,
cutoff = .90,
verbose = TRUE)
corrMatrix <- cor(Training[sapply(Training, is.numeric)])
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1],
col = 1:dim(corrMatrix)[2])
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
High_cor_var = findCorrelation(corrMatrix,
cutoff = .90,
verbose = TRUE)
Training = Training[, -High_cor_var]
dim(Training)
inTrain <- createDataPartition(y=Training$classe, p=.7, list=FALSE)
Training_Data <- Training[inTrain,]
Testing_Data <- Training[-inTrain,]
dim(Training_Data);dim(Testing_Data)
inTrain <- createDataPartition(y=Training$classe, p=.7, list=FALSE)
Training_Data <- Training[inTrain,]
Testing_Data <- Training[-inTrain,]
dim(Training_Data);dim(Testing_Data)
library(tree)
set.seed(12345)
Model_Tree=tree(classe~.,data=Training_Data)
summary(Model_Tree)
fancyRpartPlot(Model_Tree)
library(rpart)
fancyRpartPlot(Model_Tree)
library(rpart.plot)
install.packages(rpart.plot)
install.packages('rpart.plot')
install.packages('rpart.plot')
library(rpart.plot)
fancyRpartPlot(Model_Tree)
??fancyRpartPlot
library(rattle)
install.packages('rattle')
fancyRpartPlot(Model_Tree)
library(rattle)
install.packages('RGtk2')
library(rpart)
library(rpart.plot)
library(rattle)
instll
install.packages('RGtk2')
remove.packages("RGtk2")
remove.packages("rattle")
install.packages("rattle")
library(rattle)
require(RGtk2)
install.packages(RGtk2)
install.packages('RGtk2')
plot(Model_Tree$finalModel,uniform=TRUE,main = "classification tree")
text(Model_Tree$finalModel,use.n=TRUE, all=TRUE,cex=.8)
summary(Model_Tree)
plot(Model_Tree)
plot(Model_Tree, uniform=TRUE,main = "classification tree")
plot(Model_Tree, main = "classification tree")
plot(Model_Tree)
plot(Model_Tree, uniform=TRUE)
text(Model_Tree,use.n=TRUE, all=TRUE,cex=.8)
text(Model_Tree, cex=.8)
text(Model_Tree,pretty = 0, cex=.8)
text(Model_Tree,pretty = 1, cex=.8)
text(Model_Tree,pretty = 10, cex=.8)
text(Model_Tree,pretty = 0, cex=.8)
modFitDT <- rpart(classe ~ .,
data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
Plot(modFitDT)
plot(modFitDT)
set.seed(12345)
prediction <- predict(modFitDT, Testing_Data, type = "class")
confusionMatrix(prediction, Testing_Data$classe)
prediction <- predict(modFitDT, Training_Data, type = "class")
confusionMatrix(prediction, Training_Data$classe)
prediction1 <-predict(Model_Tree, Training_Data, type = "class")
c1 <- confusionMatrix(prediction1, Training_Data$classe)
c1
library(rpart)
modFitDT <- rpart(classe ~ .,
data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
plot(Model_Tree)
text(Model_Tree,pretty = 0, cex=.8)
text(Model_Tree,cex=.8)
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/freefrog/Studing/DataScience/Practical_ML')
Train_Data <- read.csv('pml-training.csv',na.strings=c("", "NA", "NULL"))
Test_Data <- read.csv('pml-testing.csv', na.strings=c("", "NA", "NULL"))
dim(Train_Data);dim(Test_Data)
# Remove variables that contain NA values
Training <- Train_Data[,colSums(is.na(Train_Data))==0]
dim(Training)
# Remove unrelated variables
remove = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
Training <- Training[,-which(names(Training) %in% remove)]
dim(Training)
# Remove low variance variables
library(caret)
zeroVar= nearZeroVar(Training[,sapply(Training, is.numeric)],
saveMetrics = TRUE)
Training = Training[,zeroVar[, 'nzv']==0]
dim(Training)
# Remove highly correlated variables (90%)
corrMatrix <- cor(Training[sapply(Training, is.numeric)])
corrDF <- expand.grid(row = 1:dim(corrMatrix)[1],
col = 1:dim(corrMatrix)[2])
corrDF$correlation <- as.vector(corrMatrix)
levelplot(correlation ~ row+ col, corrDF)
High_cor_var = findCorrelation(corrMatrix,
cutoff = .90,
verbose = TRUE)
Training = Training[, -High_cor_var]
dim(Training)
inTrain <- createDataPartition(y=Training$classe, p=.7, list=FALSE)
Training_Data <- Training[inTrain,]
Testing_Data <- Training[-inTrain,]
dim(Training_Data);dim(Testing_Data)
library(rpart)
modFitDT <- rpart(classe ~ .,
data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
plot(Model_Tree)
text(Model_Tree,cex=.8)
modFitDT <- rpart(classe ~ ., data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
library(rpart)
modFitDT <- rpart(classe ~ ., data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
# plot(Model_Tree)
# text(Model_Tree,cex=.8)
library(rpart)
modFitDT <- rpart(classe ~ ., data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
plot(Model_Tree)
text(Model_Tree,cex=.8)
library(rpart)
modFitDT <- rpart(classe ~ ., data = Training_Data,
method="class",
control = rpart.control(method = "cv",
number = 10))
plot(Model_Tree)
text(Model_Tree,cex=.8)
prediction <- predict(modFitDT, Testing_Data, type = "class")
confusionMatrix(prediction, Testing_Data$classe)
summary(modFitDT)
library(rattle)
install.packages('RGtk2')
